{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HammingIMQKernel Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually Import HammingIMQ Kernel due to API missing implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from gpytorch.constraints.constraints import Interval, Positive\n",
    "from gpytorch.kernels.kernel import Kernel\n",
    "from gpytorch.priors.prior import Prior\n",
    "\n",
    "\n",
    "EMPTY_SIZE = torch.Size([])\n",
    "\n",
    "\n",
    "class HammingIMQKernel(Kernel):\n",
    "    r\"\"\"\n",
    "    Computes a covariance matrix based on the inverse multiquadratic Hamming kernel\n",
    "    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
    "\n",
    "    .. math::\n",
    "       \\begin{equation*}\n",
    "            k_{\\text{H-IMQ}}(\\mathbf{x_1}, \\mathbf{x_2}) =\n",
    "            \\left( \\frac{1 + \\alpha}{\\alpha + d_{\\text{Hamming}}(x1, x2)} \\right)^\\beta\n",
    "       \\end{equation*}\n",
    "    where :math:`\\alpha` and :math:`\\beta` are strictly positive scale parameters.\n",
    "    This kernel was proposed in `Biological Sequence Kernels with Guaranteed Flexibility`.\n",
    "    See http://arxiv.org/abs/2304.03775 for more details.\n",
    "\n",
    "    This kernel is meant to be used for fixed-length one-hot encoded discrete sequences.\n",
    "    Because GPyTorch is particular about dimensions, the one-hot sequence encoding should be flattened\n",
    "    to a vector with length :math:`T \\times V`, where :math:`T` is the sequence length and :math:`V` is the\n",
    "    vocabulary size.\n",
    "\n",
    "    :param vocab_size: The size of the vocabulary.\n",
    "    :param batch_shape: Set this if you want a separate kernel hyperparameters for each batch of input\n",
    "        data. It should be :math:`B_1 \\times \\ldots \\times B_k` if :math:`\\mathbf{x_1}` is\n",
    "        a :math:`B_1 \\times \\ldots \\times B_k \\times N \\times D` tensor.\n",
    "    :param alpha_prior: Set this if you want to apply a prior to the\n",
    "        alpha parameter.\n",
    "    :param: alpha_constraint: Set this if you want to apply a constraint\n",
    "        to the alpha parameter. If None is passed, the default is `Positive()`.\n",
    "    :param beta_prior: Set this if you want to apply a prior to the\n",
    "        beta parameter.\n",
    "    :param beta_constraint: Set this if you want to apply a constraint\n",
    "        to the beta parameter. If None is passed, the default is `Positive()`.\n",
    "\n",
    "    Example:\n",
    "        >>> vocab_size = 8\n",
    "        >>> x_cat = torch.tensor([[7, 7, 7, 7], [5, 7, 3, 4]])  # batch_size x seq_length\n",
    "        >>> x_one_hot = F.one_hot(x_cat, num_classes=vocab_size)  # batch_size x seq_length x vocab_size\n",
    "        >>> x_flat = x_one_hot.view(*x_cat.shape[:-1], -1)  # batch_size x (seq_length * vocab_size)\n",
    "        >>> covar_module = gpytorch.kernels.HammingIMQKernel(vocab_size=vocab_size)\n",
    "        >>> covar = covar_module(x_flat)  # Output: LinearOperator of size (2 x 2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        batch_shape: torch.Size = EMPTY_SIZE,\n",
    "        alpha_prior: Optional[Prior] = None,\n",
    "        alpha_constraint: Optional[Interval] = None,\n",
    "        beta_prior: Optional[Prior] = None,\n",
    "        beta_constraint: Optional[Interval] = None,\n",
    "    ):\n",
    "        super().__init__(batch_shape=batch_shape)\n",
    "        self.vocab_size = vocab_size\n",
    "        # add alpha (scale) parameter\n",
    "        alpha_constraint = Positive() if alpha_constraint is None else alpha_constraint\n",
    "        self.register_parameter(\n",
    "            name=\"raw_alpha\",\n",
    "            parameter=nn.Parameter(torch.zeros(*self.batch_shape, 1)),\n",
    "        )\n",
    "        if alpha_prior is not None:\n",
    "            self.register_prior(\"alpha_prior\", alpha_prior, self._alpha_param, self._alpha_closure)\n",
    "        self.register_constraint(\"raw_alpha\", alpha_constraint)\n",
    "\n",
    "        # add beta parameter\n",
    "        beta_constraint = Positive() if beta_constraint is None else beta_constraint\n",
    "        self.register_parameter(\n",
    "            name=\"raw_beta\",\n",
    "            parameter=nn.Parameter(torch.zeros(*self.batch_shape, 1)),\n",
    "        )\n",
    "        if beta_prior is not None:\n",
    "            self.register_prior(\"beta_prior\", beta_prior, self._beta_param, self._beta_closure)\n",
    "        self.register_constraint(\"raw_beta\", beta_constraint)\n",
    "\n",
    "    @property\n",
    "    def alpha(self) -> Tensor:\n",
    "        return self.raw_alpha_constraint.transform(self.raw_alpha)\n",
    "\n",
    "    @alpha.setter\n",
    "    def alpha(self, value: Tensor):\n",
    "        self._set_alpha(value)\n",
    "\n",
    "    def _alpha_param(self, m: Kernel) -> Tensor:\n",
    "        # Used by the alpha_prior\n",
    "        return m.alpha\n",
    "\n",
    "    def _alpha_closure(self, m: Kernel, v: Tensor) -> Tensor:\n",
    "        # Used by the alpha_prior\n",
    "        return m._set_alpha(v)\n",
    "\n",
    "    def _set_alpha(self, value: Tensor):\n",
    "        # Used by the alpha_prior\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_alpha)\n",
    "        self.initialize(raw_alpha=self.raw_alpha_constraint.inverse_transform(value))\n",
    "\n",
    "    @property\n",
    "    def beta(self) -> Tensor:\n",
    "        return self.raw_beta_constraint.transform(self.raw_beta)\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, value: Tensor):\n",
    "        self._set_beta(value)\n",
    "\n",
    "    def _beta_param(self, m: Kernel) -> Tensor:\n",
    "        # Used by the beta_prior\n",
    "        return m.beta\n",
    "\n",
    "    def _beta_closure(self, m: Kernel, v: Tensor) -> Tensor:\n",
    "        # Used by the beta_prior\n",
    "        return m._set_beta(v)\n",
    "\n",
    "    def _set_beta(self, value: Tensor):\n",
    "        # Used by the beta_prior\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_beta)\n",
    "        self.initialize(raw_beta=self.raw_beta_constraint.inverse_transform(value))\n",
    "\n",
    "    def _imq(self, dist: Tensor) -> Tensor:\n",
    "        return ((1 + self.alpha) / (self.alpha + dist)).pow(self.beta)\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor, diag: bool = False, **params):\n",
    "        # GPyTorch is pretty particular about dimensions so we need to unflatten the one-hot encoding\n",
    "        x1 = x1.view(*x1.shape[:-1], -1, self.vocab_size)\n",
    "        x2 = x2.view(*x2.shape[:-1], -1, self.vocab_size)\n",
    "\n",
    "        x1_eq_x2 = torch.equal(x1, x2)\n",
    "\n",
    "        if diag:\n",
    "            if x1_eq_x2:\n",
    "                res = ((1 + self.alpha) / self.alpha).pow(self.beta)\n",
    "                skip_dims = [-1] * len(self.batch_shape)\n",
    "                return res.expand(*skip_dims, x1.size(-3))\n",
    "            else:\n",
    "                dist = x1.size(-2) - (x1 * x2).sum(dim=(-1, -2))\n",
    "                return self._imq(dist)\n",
    "\n",
    "        else:\n",
    "            dist = hamming_dist(x1, x2, x1_eq_x2)\n",
    "\n",
    "        return self._imq(dist)\n",
    "\n",
    "\n",
    "def hamming_dist(x1: Tensor, x2: Tensor, x1_eq_x2: bool) -> Tensor:\n",
    "    res = x1.size(-2) - (x1.unsqueeze(-3) * x2.unsqueeze(-4)).sum(dim=(-1, -2))\n",
    "    if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:\n",
    "        res.diagonal(dim1=-2, dim2=-1).fill_(0)\n",
    "    # Zero out negative values\n",
    "    print(res.clamp_min_(0))\n",
    "    return res.clamp_min_(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the data (categories) and call it x_cat. Then, we run the IMQ-H over the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "vocab_size = 8\n",
    "x_cat = torch.tensor([[3, 2, 1, 7, 5], [3, 2, 3, 4, 2]])  # batch_size x seq_length\n",
    "x_one_hot = F.one_hot(x_cat, num_classes=vocab_size)  # batch_size x seq_length x vocab_size\n",
    "x_flat = x_one_hot.view(*x_cat.shape[:-1], -1)  # batch_size x (seq_length * vocab_size)\n",
    "covar_module = HammingIMQKernel(vocab_size=vocab_size)\n",
    "covar = covar_module(x_flat)  # Output: LinearOperator of size (2 x 2) \n",
    "                              # Note that LinearOperators are simply transformation (mapping) matrices!              "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we print out our one-hot encodings to make sure our categories are correctly visualized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print out the Hamming distance and covariance matrices, which we can use to assess the similarity between our sequences.\n",
    "Note: A higher covariance indicates increasing similarity (covariance is measured relatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3],\n",
      "        [3, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.857165 , 0.5824112],\n",
       "       [0.5824112, 1.857165 ]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the IMQ-H does not detect how similar the sequences are, only how different. Here, we see that despite having a higher ratio of matching digits vs non-matching digits within the sequences, our covariance matrix does not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = torch.tensor([[3, 2, 1, 7, 5, 7], [3, 2, 3, 4, 2, 7]])  # Here, we add an extra pair of 7's to the end.\n",
    "                                                                # This brings the ratio of matching to non-matching\n",
    "                                                                # from 3:2 --> 3:3 = 1:1\n",
    "\n",
    "x_one_hot = F.one_hot(x_cat, num_classes=vocab_size) \n",
    "x_flat = x_one_hot.view(*x_cat.shape[:-1], -1)  \n",
    "covar_module = HammingIMQKernel(vocab_size=vocab_size)\n",
    "covar = covar_module(x_flat)  # Output: LinearOperator of size (2 x 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]],\n",
       "\n",
       "        [[0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3],\n",
      "        [3, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.857165 , 0.5824112],\n",
       "       [0.5824112, 1.857165 ]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the covariance matrix did not change! This is because we find the covariance of the Hamming distance LinearOperator, which remains unchanged!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
